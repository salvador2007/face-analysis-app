# -*- coding: utf-8 -*-
import tensorflow as tf
import keras
from keras import layers
import numpy as np
import os
from datetime import datetime
import matplotlib.pyplot as plt
import json
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
from PIL import Image
import cv2
import warnings
import logging
import shutil
import hashlib
import gc

warnings.filterwarnings('ignore')

# ========== ë¡œê¹… ê°œì„  ==============
def setup_logging():
    """ë¡œê¹… ì„¤ì • ê°œì„ """
    # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ì œê±°
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)
    
    # ìƒˆë¡œìš´ ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),  # ì½˜ì†” ì¶œë ¥
        ]
    )
    return logging.getLogger(__name__)

logger = setup_logging()

# ========== GPU ìµœì í™” í´ë˜ìŠ¤ ê°œì„  ==============
class GPUOptimizer:
    @staticmethod
    def setup_gpu_optimized(memory_limit: int = 6144) -> bool:  # ë©”ëª¨ë¦¬ ì œí•œ ê°ì†Œ
        try:
            gpus = tf.config.list_physical_devices('GPU')
            if gpus:
                gpu = gpus[0]
                # ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš©
                tf.config.experimental.set_memory_growth(gpu, True)
                
                # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •
                tf.config.set_logical_device_configuration(
                    gpu,
                    [tf.config.LogicalDeviceConfiguration(memory_limit=memory_limit)]
                )
                
                # Mixed Precision ì„¤ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì¦ëŒ€)
                policy = tf.keras.mixed_precision.Policy('mixed_float16')
                tf.keras.mixed_precision.set_global_policy(policy)
                
                logger.info(f"âœ… GPU ìµœì í™” ì™„ë£Œ: {memory_limit}MB ì œí•œ")
                logger.info(f"âœ… Mixed Precision í™œì„±í™”: {policy.name}")
                
                # GPU í…ŒìŠ¤íŠ¸
                with tf.device('/GPU:0'):
                    test_tensor = tf.constant([1.0, 2.0, 3.0])
                    result = tf.reduce_sum(test_tensor).numpy()
                    logger.info(f"âœ… GPU í…ŒìŠ¤íŠ¸ ì„±ê³µ: {result}")
                return True
            else:
                logger.info("â„¹ï¸ GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ í•™ìŠµí•©ë‹ˆë‹¤.")
                return False
        except Exception as e:
            logger.error(f"âš ï¸ GPU ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
            logger.info("CPU ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            return False

    @staticmethod
    def check_tensorflow_gpu() -> bool:
        logger.info("\nğŸ” TensorFlow GPU ì„¤ì • í™•ì¸:")
        logger.info("=" * 40)
        logger.info(f"TensorFlow ë²„ì „: {tf.__version__}")
        
        gpu_devices = tf.config.list_physical_devices('GPU')
        logger.info(f"GPU ì¥ì¹˜: {len(gpu_devices)}ê°œ")
        
        if gpu_devices:
            for i, gpu in enumerate(gpu_devices):
                logger.info(f"  GPU {i}: {gpu}")
                
        logger.info(f"CUDA ì§€ì›: {tf.test.is_built_with_cuda()}")
        logger.info(f"GPU ì‚¬ìš© ê°€ëŠ¥: {len(gpu_devices) > 0}")
        return len(gpu_devices) > 0

# ========== ëª¨ë¸ ë¹Œë” í´ë˜ìŠ¤ ê°œì„  ==============
class ModelBuilder:
    @staticmethod
    def create_efficient_model(input_shape=(224,224,3), num_classes=7, dropout_rate=0.3):
        """ë” ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ì¸ ëª¨ë¸ êµ¬ì¡°"""
        inputs = keras.Input(shape=input_shape, name='face_input')
        
        # ë°ì´í„° ì •ê·œí™”ë¥¼ ë³„ë„ ë ˆì´ì–´ë¡œ ë¶„ë¦¬
        x = layers.Rescaling(1./255)(inputs)
        
        # ë°ì´í„° ì¦ê°• (í›ˆë ¨ì‹œì—ë§Œ ì ìš©)
        x = layers.RandomFlip('horizontal')(x)
        x = layers.RandomRotation(0.1)(x)
        x = layers.RandomZoom(0.1)(x)
        x = layers.RandomBrightness(0.1)(x)
        x = layers.RandomContrast(0.1)(x)
        
        # ë¸”ë¡1 - ê°€ë²¼ìš´ ì‹œì‘
        x = layers.Conv2D(32, 3, padding='same', kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Conv2D(32, 3, padding='same', kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.MaxPooling2D(2)(x)
        x = layers.Dropout(dropout_rate * 0.5)(x)
        
        # ë¸”ë¡2
        x = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Conv2D(64, 3, padding='same', kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.MaxPooling2D(2)(x)
        x = layers.Dropout(dropout_rate)(x)
        
        # ë¸”ë¡3
        x = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Conv2D(128, 3, padding='same', kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.MaxPooling2D(2)(x)
        x = layers.Dropout(dropout_rate)(x)
        
        # ë¸”ë¡4 - ë” íš¨ìœ¨ì ì¸ êµ¬ì¡°
        x = layers.Conv2D(256, 3, padding='same', kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.GlobalAveragePooling2D()(x)
        
        # Dense ë ˆì´ì–´ ê°„ì†Œí™”
        x = layers.Dense(256, kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Dropout(dropout_rate * 1.5)(x)
        
        x = layers.Dense(128, kernel_initializer='he_normal')(x)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        x = layers.Dropout(dropout_rate)(x)
        
        # Output layer - Mixed Precision í˜¸í™˜ì„± í™•ë³´
        outputs = layers.Dense(
            num_classes, 
            activation='softmax', 
            kernel_initializer='glorot_uniform', 
            dtype='float32',  # ëª…ì‹œì  float32 ì„¤ì •
            name='predictions'
        )(x)
        
        model = keras.Model(inputs, outputs, name='EfficientFaceClassifier')
        return model

# ========== ë°ì´í„° ê´€ë¦¬ í´ë˜ìŠ¤ ê°œì„  ==============
class DataManager:
    def __init__(self, data_dir: str):
        self.data_dir = os.path.abspath(data_dir)
        self.supported_formats = ('.png', '.jpg', '.jpeg', '.bmp', '.tiff', '.webp')
    
    def validate_data_structure(self):
        """ë°ì´í„° êµ¬ì¡° ê²€ì¦ ê°œì„ """
        if not os.path.exists(self.data_dir):
            logger.error(f"âŒ ë°ì´í„° ë””ë ‰í„°ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.data_dir}")
            return False, 0, {}
        
        logger.info("\nğŸ“Š ë°ì´í„°ì…‹ êµ¬ì¡° ë¶„ì„:")
        logger.info("=" * 50)
        
        total_images = 0
        class_counts = {}
        
        # ìœ íš¨í•œ ë””ë ‰í„°ë¦¬ë§Œ ì²˜ë¦¬
        valid_dirs = [d for d in os.listdir(self.data_dir) 
                     if os.path.isdir(os.path.join(self.data_dir, d)) and not d.startswith('.')]
        
        if not valid_dirs:
            logger.error("âŒ ìœ íš¨í•œ í´ë˜ìŠ¤ ë””ë ‰í„°ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return False, 0, {}
        
        for class_name in sorted(valid_dirs):
            class_path = os.path.join(self.data_dir, class_name)
            valid_images = []
            
            try:
                files = os.listdir(class_path)
                for file in files:
                    if file.lower().endswith(self.supported_formats):
                        file_path = os.path.join(class_path, file)
                        if self._is_valid_image(file_path):
                            valid_images.append(file_path)
            except Exception as e:
                logger.warning(f"âš ï¸ í´ë˜ìŠ¤ {class_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
            
            count = len(valid_images)
            class_counts[class_name] = count
            total_images += count
            logger.info(f"  ğŸ“ {class_name}: {count:4d}ê°œ ì´ë¯¸ì§€")
        
        logger.info(f"\nğŸ“ˆ ì´ ì´ë¯¸ì§€ ìˆ˜: {total_images:,}ê°œ")
        logger.info(f"ğŸ“ í´ë˜ìŠ¤ ìˆ˜: {len(class_counts)}ê°œ")
        
        if not class_counts or total_images == 0:
            logger.error("âŒ ìœ íš¨í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return False, 0, {}
        
        self._analyze_class_imbalance(class_counts)
        return True, len(class_counts), class_counts
    
    def _is_valid_image(self, file_path: str) -> bool:
        """ì´ë¯¸ì§€ ìœ íš¨ì„± ê²€ì‚¬ ê°œì„ """
        try:
            # íŒŒì¼ í¬ê¸° ì²´í¬
            if os.path.getsize(file_path) < 1024:  # 1KB ë¯¸ë§Œì€ ì œì™¸
                return False
            
            # PILë¡œ ì´ë¯¸ì§€ ê²€ì¦
            with Image.open(file_path) as img:
                img.verify()
                
            # ë‹¤ì‹œ ì—´ì–´ì„œ ê¸°ë³¸ ì •ë³´ í™•ì¸
            with Image.open(file_path) as img:
                width, height = img.size
                if width < 32 or height < 32:  # ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ ì œì™¸
                    return False
                    
            return True
        except Exception:
            return False
    
    def _analyze_class_imbalance(self, class_counts):
        """í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¶„ì„"""
        counts = list(class_counts.values())
        if not counts:
            return
            
        min_count = min(counts)
        max_count = max(counts)
        avg_count = sum(counts) / len(counts)
        
        logger.info(f"\nğŸ“Š í´ë˜ìŠ¤ë³„ ë¶„í¬:")
        logger.info(f"   ìµœì†Œ: {min_count}ê°œ")
        logger.info(f"   ìµœëŒ€: {max_count}ê°œ")
        logger.info(f"   í‰ê· : {avg_count:.1f}ê°œ")
        
        if min_count > 0:
            imbalance_ratio = max_count / min_count
            logger.info(f"   ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.2f}:1")
            
            if imbalance_ratio > 3:
                logger.warning("âš ï¸ ì‹¬ê°í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°ì§€!")
            elif imbalance_ratio > 2:
                logger.warning("âš ï¸ ì•½ê°„ì˜ í´ë˜ìŠ¤ ë¶ˆê· í˜• ê°ì§€")
        
        if min_count < 10:
            logger.warning(f"âš ï¸ ê²½ê³ : ì¼ë¶€ í´ë˜ìŠ¤ì˜ ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤ (ìµœì†Œ {min_count}ê°œ)")

# ========== ì´ë¯¸ì§€ ì œë„ˆë ˆì´í„° í´ë˜ìŠ¤ ê°œì„  ==============
class ImprovedImageGenerator(tf.keras.utils.Sequence):
    def __init__(self, image_paths, labels, class_names, batch_size, 
                 target_size=(224,224), augment=False, class_weights=None):
        self.image_paths = image_paths
        self.labels = labels
        self.class_names = class_names
        self.batch_size = batch_size
        self.target_size = target_size
        self.augment = augment
        self.num_classes = len(class_names)
        self.samples = len(image_paths)
        self.class_weights = class_weights or self._calculate_class_weights()
        self.indices = np.arange(len(self.image_paths))
        
        # ìºì‹œ í¬ê¸° ì œí•œ
        self.cache_size = min(50, len(image_paths) // 10)  # ìºì‹œ í¬ê¸° ê°ì†Œ
        self.image_cache = {}
        
        if augment:
            np.random.shuffle(self.indices)
    
    def _calculate_class_weights(self):
        """í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        try:
            unique_labels = np.unique(self.labels)
            class_weights = compute_class_weight(
                'balanced', classes=unique_labels, y=self.labels
            )
            return dict(zip(unique_labels, class_weights))
        except Exception as e:
            logger.warning(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {}
    
    def __len__(self):
        return int(np.ceil(len(self.image_paths) / self.batch_size))
    
    def __getitem__(self, idx):
        start_idx = idx * self.batch_size
        end_idx = min((idx + 1) * self.batch_size, len(self.image_paths))
        batch_indices = self.indices[start_idx:end_idx]
        
        batch_x = []
        batch_y = []
        
        for i in batch_indices:
            try:
                img_array = self._load_and_preprocess_image(i)
                if img_array is not None:
                    if self.augment:
                        img_array = self._augment_image(img_array)
                    
                    # ì •ê·œí™”ëŠ” ëª¨ë¸ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” 0-255 ìœ ì§€
                    img_array = img_array.astype(np.float32)
                    label = tf.keras.utils.to_categorical(self.labels[i], num_classes=self.num_classes)
                    
                    batch_x.append(img_array)
                    batch_y.append(label)
                else:
                    # ì‹¤íŒ¨í•œ ê²½ìš° ê¸°ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©
                    batch_x.append(self._get_default_image())
                    batch_y.append(tf.keras.utils.to_categorical(0, num_classes=self.num_classes))
                    
            except Exception as e:
                logger.warning(f"âš ï¸ ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {self.image_paths[i]} - {e}")
                batch_x.append(self._get_default_image())
                batch_y.append(tf.keras.utils.to_categorical(0, num_classes=self.num_classes))
        
        # ë°°ì¹˜ê°€ ë¹„ì–´ìˆì§€ ì•Šë„ë¡ ë³´ì¥
        if not batch_x:
            batch_x = [self._get_default_image()]
            batch_y = [tf.keras.utils.to_categorical(0, num_classes=self.num_classes)]
        
        return np.array(batch_x, dtype=np.float32), np.array(batch_y, dtype=np.float32)
    
    def _load_and_preprocess_image(self, idx):
        """ì´ë¯¸ì§€ ë¡œë“œ ë° ì „ì²˜ë¦¬ ê°œì„ """
        image_path = self.image_paths[idx]
        cache_key = hashlib.md5(image_path.encode()).hexdigest()[:16]  # ì§§ì€ í•´ì‹œ
        
        # ìºì‹œ í™•ì¸
        if cache_key in self.image_cache:
            return self.image_cache[cache_key].copy()
        
        try:
            # OpenCVë¡œ ì´ë¯¸ì§€ ë¡œë“œ ì‹œë„
            img_array = cv2.imdecode(
                np.fromfile(image_path, dtype=np.uint8), 
                cv2.IMREAD_COLOR
            )
            
            if img_array is None:
                # PILë¡œ ëŒ€ì²´ ì‹œë„
                with Image.open(image_path) as img:
                    if img.mode != 'RGB':
                        img = img.convert('RGB')
                    img_array = np.array(img)
            else:
                # BGR to RGB ë³€í™˜
                img_array = cv2.cvtColor(img_array, cv2.COLOR_BGR2RGB)
            
            # ë¦¬ì‚¬ì´ì¦ˆ
            img_array = cv2.resize(img_array, self.target_size, interpolation=cv2.INTER_AREA)
            
            # ìºì‹œì— ì €ì¥ (ì œí•œëœ ìˆ˜ë§Œ)
            if len(self.image_cache) < self.cache_size:
                self.image_cache[cache_key] = img_array.copy()
            
            return img_array
            
        except Exception as e:
            logger.warning(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {image_path} - {e}")
            return None
    
    def _get_default_image(self):
        """ê¸°ë³¸ ì´ë¯¸ì§€ ìƒì„±"""
        return np.zeros((*self.target_size, 3), dtype=np.float32)
    
    def _augment_image(self, image):
        """ì´ë¯¸ì§€ ì¦ê°• (ê°€ë²¼ìš´ ë²„ì „)"""
        image = image.copy()
        
        # ìˆ˜í‰ ë’¤ì§‘ê¸°
        if np.random.random() > 0.5:
            image = np.fliplr(image)
        
        # ë°ê¸° ì¡°ì •
        if np.random.random() > 0.5:
            brightness_factor = np.random.uniform(0.8, 1.2)
            image = np.clip(image * brightness_factor, 0, 255)
        
        # ëŒ€ë¹„ ì¡°ì •
        if np.random.random() > 0.7:
            contrast_factor = np.random.uniform(0.8, 1.2)
            image = np.clip((image - 128) * contrast_factor + 128, 0, 255)
        
        return image.astype(np.float32)
    
    def on_epoch_end(self):
        """ì—í¬í¬ ì¢…ë£Œì‹œ ì…”í”Œ"""
        if self.augment:
            np.random.shuffle(self.indices)
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if len(self.image_cache) > self.cache_size * 2:
            # ìºì‹œ ì ˆë°˜ ì œê±°
            keys_to_remove = list(self.image_cache.keys())[::2]
            for key in keys_to_remove:
                del self.image_cache[key]
            gc.collect()

# ========== í•™ìŠµ ë§¤ë‹ˆì € í´ë˜ìŠ¤ ê°œì„  ==============
class TrainingManager:
    def __init__(self, model_name: str = 'face_shape_model'):
        self.model_name = model_name
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self._create_directories()
    
    def _create_directories(self):
        """í•„ìš”í•œ ë””ë ‰í„°ë¦¬ ìƒì„±"""
        directories = ['logs', 'models', 'checkpoints']
        for directory in directories:
            try:
                os.makedirs(directory, exist_ok=True)
                logger.info(f"ğŸ“ ë””ë ‰í„°ë¦¬ ìƒì„±/í™•ì¸: {directory}")
            except Exception as e:
                logger.warning(f"ë””ë ‰í„°ë¦¬ ìƒì„± ì‹¤íŒ¨: {directory} - {e}")
    
    def compile_model(self, model: keras.Model, num_classes: int):
        """ëª¨ë¸ ì»´íŒŒì¼ ê°œì„ """
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ê°œì„ 
        initial_learning_rate = 0.001
        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
            initial_learning_rate,
            decay_steps=1000,
            alpha=0.01  # ìµœì†Œ í•™ìŠµë¥  ì¦ê°€
        )
        
        # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        optimizer = tf.keras.optimizers.Adam(
            learning_rate=lr_schedule,
            beta_1=0.9,
            beta_2=0.999,
            epsilon=1e-7
        )
        
        # Mixed Precision ìµœì í™”
        if tf.keras.mixed_precision.global_policy().name == 'mixed_float16':
            optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)
        
        # ì»´íŒŒì¼
        model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy', 'top_k_categorical_accuracy']
        )
        
        logger.info("ğŸ“Š ëª¨ë¸ êµ¬ì¡°:")
        try:
            model.summary(print_fn=logger.info)
        except:
            logger.info("ëª¨ë¸ êµ¬ì¡° ì¶œë ¥ ì‹¤íŒ¨")
        
        return model
    
    def get_callbacks(self):
        """ì½œë°± ì„¤ì • ê°œì„ """
        callbacks = []
        
        try:
            # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
            checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
                filepath=os.path.join('checkpoints', f'best_{self.model_name}_{{epoch:02d}}_{{val_accuracy:.4f}}.weights.h5'),
                monitor='val_accuracy',
                save_best_only=True,
                save_weights_only=True,
                verbose=1,
                mode='max'
            )
            callbacks.append(checkpoint_callback)
            
            # ì¡°ê¸° ì¢…ë£Œ
            early_stopping = tf.keras.callbacks.EarlyStopping(
                monitor='val_loss',
                patience=10,  # patience ê°ì†Œ
                restore_best_weights=True,
                verbose=1,
                mode='min'
            )
            callbacks.append(early_stopping)
            
            # í•™ìŠµë¥  ê°ì†Œ
            reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,  # patience ê°ì†Œ
                min_lr=1e-7,
                verbose=1,
                mode='min'
            )
            callbacks.append(reduce_lr)
            
            # CSV ë¡œê±° (ê°„ë‹¨í•œ ê²½ë¡œ)
            csv_logger = tf.keras.callbacks.CSVLogger(
                filename=f'training_log_{self.timestamp}.csv',
                append=True
            )
            callbacks.append(csv_logger)
            
        except Exception as e:
            logger.warning(f"ì¼ë¶€ ì½œë°± ì„¤ì • ì‹¤íŒ¨: {e}")
        
        return callbacks
    
    def save_model(self, model: keras.Model):
        """ëª¨ë¸ ì €ì¥ ê°œì„ """
        saved_successfully = False
        
        try:
            # Keras í˜•ì‹ìœ¼ë¡œ ì €ì¥
            keras_path = os.path.join('models', f'{self.model_name}.keras')
            model.save(keras_path)
            logger.info(f"âœ… Keras ëª¨ë¸ ì €ì¥ë¨: {keras_path}")
            saved_successfully = True
        except Exception as e:
            logger.warning(f"Keras ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        try:
            # ê°€ì¤‘ì¹˜ë§Œ ì €ì¥
            weights_path = os.path.join('models', f'{self.model_name}.weights.h5')
            model.save_weights(weights_path)
            logger.info(f"âœ… ê°€ì¤‘ì¹˜ ì €ì¥ë¨: {weights_path}")
            saved_successfully = True
        except Exception as e:
            logger.warning(f"ê°€ì¤‘ì¹˜ ì €ì¥ ì‹¤íŒ¨: {e}")
        
        try:
            # ëª¨ë¸ êµ¬ì¡° JSONìœ¼ë¡œ ì €ì¥
            model_json = model.to_json()
            json_path = os.path.join('models', f'{self.model_name}_architecture.json')
            with open(json_path, 'w', encoding='utf-8') as json_file:
                json_file.write(model_json)
            logger.info(f"âœ… ëª¨ë¸ êµ¬ì¡° ì €ì¥ë¨: {json_path}")
        except Exception as e:
            logger.warning(f"ëª¨ë¸ êµ¬ì¡° ì €ì¥ ì‹¤íŒ¨: {e}")
        
        if not saved_successfully:
            logger.error("âŒ ëª¨ë“  ëª¨ë¸ ì €ì¥ ë°©ë²•ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤!")
    
    def zip_results(self):
        """ê²°ê³¼ ì••ì¶•"""
        try:
            output_zip = f'result_package_{self.timestamp}'
            if os.path.exists('models') and os.listdir('models'):
                shutil.make_archive(base_name=output_zip, format='zip', root_dir='models')
                logger.info(f"ğŸ“¦ ê²°ê³¼ ZIP ì••ì¶• ì™„ë£Œ: {output_zip}.zip")
            else:
                logger.warning("ì••ì¶•í•  ëª¨ë¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"ZIP ì••ì¶• ì‹¤íŒ¨: {e}")

# ========== ë°ì´í„° ìƒì„±ê¸° í•¨ìˆ˜ ê°œì„  ==============
def create_data_generators(data_dir, batch_size=16, validation_split=0.2, target_size=(224,224)):
    """ë°ì´í„° ì œë„ˆë ˆì´í„° ìƒì„± í•¨ìˆ˜ ê°œì„ """
    logger.info(f"ğŸ“‚ ë°ì´í„° ë””ë ‰í„°ë¦¬: {data_dir}")
    
    data_manager = DataManager(data_dir)
    is_valid, num_classes, class_counts = data_manager.validate_data_structure()
    
    if not is_valid:
        raise ValueError("ë°ì´í„° êµ¬ì¡°ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    
    # ì´ë¯¸ì§€ ê²½ë¡œì™€ ë ˆì´ë¸” ìˆ˜ì§‘
    image_paths, labels, class_names = [], [], list(class_counts.keys())
    
    for class_idx, class_name in enumerate(class_names):
        class_path = os.path.join(data_dir, class_name)
        class_images = []
        
        try:
            for file in os.listdir(class_path):
                if file.lower().endswith(data_manager.supported_formats):
                    file_path = os.path.join(class_path, file)
                    if data_manager._is_valid_image(file_path):
                        class_images.append(file_path)
            
            # í´ë˜ìŠ¤ë³„ ìµœëŒ€ ì´ë¯¸ì§€ ìˆ˜ ì œí•œ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
            if len(class_images) > 1000:
                class_images = np.random.choice(class_images, 1000, replace=False).tolist()
                logger.info(f"âš ï¸ {class_name} í´ë˜ìŠ¤: {len(class_images)}ê°œë¡œ ì œí•œ")
            
            image_paths.extend(class_images)
            labels.extend([class_idx] * len(class_images))
            
        except Exception as e:
            logger.error(f"í´ë˜ìŠ¤ {class_name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    if not image_paths:
        raise ValueError("ìœ íš¨í•œ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    logger.info(f"ğŸ“Š ì´ ìˆ˜ì§‘ëœ ì´ë¯¸ì§€: {len(image_paths)}ê°œ")
    
    # í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚°
    try:
        class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)
        class_weight_dict = dict(zip(np.unique(labels), class_weights))
    except Exception as e:
        logger.warning(f"í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
        class_weight_dict = {}
    
    # í›ˆë ¨/ê²€ì¦ ë¶„í• 
    try:
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            image_paths, labels, 
            test_size=validation_split, 
            stratify=labels, 
            random_state=42
        )
    except Exception as e:
        logger.warning(f"Stratified split ì‹¤íŒ¨, ì¼ë°˜ split ì‚¬ìš©: {e}")
        train_paths, val_paths, train_labels, val_labels = train_test_split(
            image_paths, labels, 
            test_size=validation_split, 
            random_state=42
        )
    
    logger.info(f"ğŸ“ˆ í•™ìŠµ ìƒ˜í”Œ: {len(train_paths)}ê°œ")
    logger.info(f"ğŸ“Š ê²€ì¦ ìƒ˜í”Œ: {len(val_paths)}ê°œ")
    
        # ì œë„ˆë ˆì´í„° ìƒì„±
    train_gen = ImprovedImageGenerator(
        train_paths, train_labels, class_names, batch_size,
        target_size=target_size, augment=True, class_weights=class_weight_dict
    )
    val_gen = ImprovedImageGenerator(
        val_paths, val_labels, class_names, batch_size,
        target_size=target_size, augment=False, class_weights=class_weight_dict
    )

    return train_gen, val_gen, class_names, class_weight_dict

if __name__ == "__main__":
    # ====== ì‚¬ìš©ì ì„¤ì • ======
    DATA_DIR = "./dataset/train"  # ë°ì´í„°ì…‹ ê²½ë¡œ (train í•˜ìœ„ í´ë”ë¡œ ìˆ˜ì •)
    BATCH_SIZE = 16
    VALIDATION_SPLIT = 0.2
    TARGET_SIZE = (224, 224)
    MODEL_NAME = "face_shape_model"
    EPOCHS = 30

    logger.info("\nğŸš€ [MAIN] í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")

    # 1. GPU ìµœì í™”
    GPUOptimizer.setup_gpu_optimized(memory_limit=8192)
    GPUOptimizer.check_tensorflow_gpu()

    # 2. ë°ì´í„° ìƒì„±ê¸° ì¤€ë¹„
    train_gen, val_gen, class_names, class_weight_dict = create_data_generators(
        DATA_DIR, batch_size=BATCH_SIZE, validation_split=VALIDATION_SPLIT, target_size=TARGET_SIZE
    )
    num_classes = len(class_names)

    # 3. ëª¨ë¸ ìƒì„± ë° ì»´íŒŒì¼
    model = ModelBuilder.create_efficient_model(input_shape=(*TARGET_SIZE, 3), num_classes=num_classes)
    trainer = TrainingManager(model_name=MODEL_NAME)
    model = trainer.compile_model(model, num_classes=num_classes)

    # 4. ì½œë°± ì¤€ë¹„
    callbacks = trainer.get_callbacks()

    # 5. ëª¨ë¸ í•™ìŠµ
    logger.info("\nğŸŸ¢ ëª¨ë¸ í•™ìŠµ ì‹œì‘!")
    history = model.fit(
        train_gen,
        validation_data=val_gen,
        epochs=EPOCHS,
        callbacks=callbacks,
        class_weight=class_weight_dict,
        verbose=1
    )

    # 6. ëª¨ë¸ ì €ì¥
    trainer.save_model(model)
    trainer.zip_results()

    logger.info("\nâœ… [MAIN] í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
